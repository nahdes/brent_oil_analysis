"""
Task 1: Laying the Foundation for Analysis
Brent Oil Price Change Point Analysis

This script implements the foundational analysis for detecting change points
in Brent oil prices and associating them with major geopolitical events.

Author: Birhan Energies Data Science Team
Date: February 2026

USAGE:
    Run from project root: python src/task1_analysis.py
    OR from anywhere: python task1_analysis.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
import warnings
import sys
warnings.filterwarnings('ignore')

# Set style for better visualizations
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Determine project root intelligently
if Path(__file__).parent.name == 'src':
    # Running from src/ directory
    PROJECT_ROOT = Path(__file__).parent.parent
elif (Path(__file__).parent / 'data').exists():
    # Running from project root
    PROJECT_ROOT = Path(__file__).parent
else:
    # Try to find project root by looking for data/ directory
    current = Path(__file__).parent
    while current != current.parent:
        if (current / 'data').exists():
            PROJECT_ROOT = current
            break
        current = current.parent
    else:
        PROJECT_ROOT = Path(__file__).parent

# Define project paths
DATA_RAW = PROJECT_ROOT / 'data' / 'raw'
DATA_EVENTS = PROJECT_ROOT / 'data' / 'events'
RESULTS_FIGURES = PROJECT_ROOT / 'results' / 'figures'
RESULTS_TABLES = PROJECT_ROOT / 'results' / 'tables'

# Create results directories if they don't exist
RESULTS_FIGURES.mkdir(parents=True, exist_ok=True)
RESULTS_TABLES.mkdir(parents=True, exist_ok=True)

print(f"ðŸ“ Project root: {PROJECT_ROOT}")
print(f"ðŸ“ Data directory: {DATA_RAW}")
print(f"ðŸ“ Results directory: {RESULTS_FIGURES.parent}\n")

class BrentOilAnalysis:
    """
    A class to handle Brent oil price data analysis and exploration.
    """
    
    def __init__(self, price_data_path, events_data_path):
        """
        Initialize the analysis with data paths.
        
        Parameters:
        -----------
        price_data_path : str or Path
            Path to the Brent oil prices CSV file
        events_data_path : str or Path
            Path to the major events CSV file
        """
        self.price_data_path = Path(price_data_path)
        self.events_data_path = Path(events_data_path)
        self.df_prices = None
        self.df_events = None
        
    def load_data(self):
        """
        Load and preprocess the Brent oil price data and events data.
        """
        print("=" * 80)
        print("STEP 1: DATA LOADING AND PREPROCESSING")
        print("=" * 80)
        
        # Check if files exist
        if not self.price_data_path.exists():
            raise FileNotFoundError(
                f"\nâŒ Price data file not found!\n"
                f"   Looking for: {self.price_data_path}\n"
                f"   Please copy BrentOilPrices.csv to {self.price_data_path.parent}/"
            )
        if not self.events_data_path.exists():
            raise FileNotFoundError(
                f"\nâŒ Events data file not found!\n"
                f"   Looking for: {self.events_data_path}\n"
                f"   Please copy Major_Oil_Events_Enhanced.csv to {self.events_data_path.parent}/"
            )
        
        # Load price data
        print(f"\nLoading Brent oil prices from: {self.price_data_path}")
        self.df_prices = pd.read_csv(self.price_data_path)
        
        # Convert date column to datetime (handle mixed formats)
        self.df_prices['Date'] = pd.to_datetime(self.df_prices['Date'], format='mixed', dayfirst=True)
        
        # Sort by date
        self.df_prices = self.df_prices.sort_values('Date').reset_index(drop=True)
        
        # Load events data
        print(f"Loading major events from: {self.events_data_path}")
        self.df_events = pd.read_csv(self.events_data_path)
        self.df_events['Date'] = pd.to_datetime(self.df_events['Date'])
        
        print(f"\nâœ“ Price data loaded: {len(self.df_prices):,} daily observations")
        print(f"  Date range: {self.df_prices['Date'].min().date()} to {self.df_prices['Date'].max().date()}")
        print(f"\nâœ“ Events data loaded: {len(self.df_events)} major events")
        
        # Check for missing values
        missing_prices = self.df_prices['Price'].isna().sum()
        print(f"\nMissing price values: {missing_prices}")
        
        if missing_prices > 0:
            print(f"  â†’ Handling missing values with forward fill")
            self.df_prices['Price'] = self.df_prices['Price'].fillna(method='ffill')
        
        return self
    
    def basic_statistics(self):
        """
        Calculate and display basic statistics of the price data.
        """
        print("\n" + "=" * 80)
        print("STEP 2: BASIC STATISTICAL SUMMARY")
        print("=" * 80)
        
        stats = self.df_prices['Price'].describe()
        print("\nPrice Statistics (USD per barrel):")
        print("-" * 40)
        print(f"Count:        {stats['count']:>10.0f}")
        print(f"Mean:         ${stats['mean']:>10.2f}")
        print(f"Std Dev:      ${stats['std']:>10.2f}")
        print(f"Min:          ${stats['min']:>10.2f}")
        print(f"25th %ile:    ${stats['25%']:>10.2f}")
        print(f"Median:       ${stats['50%']:>10.2f}")
        print(f"75th %ile:    ${stats['75%']:>10.2f}")
        print(f"Max:          ${stats['max']:>10.2f}")
        
        # Calculate additional metrics
        price_range = stats['max'] - stats['min']
        coef_variation = (stats['std'] / stats['mean']) * 100
        
        print(f"\nAdditional Metrics:")
        print(f"Price Range:  ${price_range:.2f}")
        print(f"Coefficient of Variation: {coef_variation:.2f}%")
        
        return self
    
    def calculate_returns(self):
        """
        Calculate daily returns and log returns.
        """
        print("\n" + "=" * 80)
        print("STEP 3: CALCULATING RETURNS")
        print("=" * 80)
        
        # Simple returns
        self.df_prices['Returns'] = self.df_prices['Price'].pct_change()
        
        # Log returns (better for statistical analysis)
        self.df_prices['Log_Returns'] = np.log(self.df_prices['Price'] / self.df_prices['Price'].shift(1))
        
        print("\nâœ“ Daily returns calculated")
        print(f"  Mean daily return: {self.df_prices['Returns'].mean()*100:.4f}%")
        print(f"  Std dev of returns: {self.df_prices['Returns'].std()*100:.4f}%")
        
        print("\nâœ“ Log returns calculated")
        print(f"  Mean log return: {self.df_prices['Log_Returns'].mean()*100:.4f}%")
        print(f"  Std dev of log returns: {self.df_prices['Log_Returns'].std()*100:.4f}%")
        
        return self
    
    def test_stationarity(self):
        """
        Perform Augmented Dickey-Fuller test for stationarity.
        """
        print("\n" + "=" * 80)
        print("STEP 4: STATIONARITY TESTING")
        print("=" * 80)
        
        try:
            from statsmodels.tsa.stattools import adfuller
            
            # Test on price levels
            print("\nAugmented Dickey-Fuller Test on Price Levels:")
            print("-" * 60)
            result_price = adfuller(self.df_prices['Price'].dropna())
            
            print(f"ADF Statistic: {result_price[0]:.6f}")
            print(f"p-value: {result_price[1]:.6f}")
            print(f"Critical Values:")
            for key, value in result_price[4].items():
                print(f"  {key}: {value:.6f}")
            
            if result_price[1] < 0.05:
                print("â†’ Result: Price series IS stationary (reject null hypothesis)")
            else:
                print("â†’ Result: Price series is NOT stationary (fail to reject null hypothesis)")
            
            # Test on log returns
            print("\nAugmented Dickey-Fuller Test on Log Returns:")
            print("-" * 60)
            result_returns = adfuller(self.df_prices['Log_Returns'].dropna())
            
            print(f"ADF Statistic: {result_returns[0]:.6f}")
            print(f"p-value: {result_returns[1]:.6f}")
            print(f"Critical Values:")
            for key, value in result_returns[4].items():
                print(f"  {key}: {value:.6f}")
            
            if result_returns[1] < 0.05:
                print("â†’ Result: Log returns ARE stationary (reject null hypothesis)")
            else:
                print("â†’ Result: Log returns are NOT stationary (fail to reject null hypothesis)")
                
        except ImportError:
            print("\nâš  statsmodels not available - skipping formal stationarity tests")
            print("  Note: Visual inspection and domain knowledge suggest:")
            print("  - Price levels are likely non-stationary (trending)")
            print("  - Log returns are likely stationary")
            print("  To install: pip install statsmodels")
        
        return self
    
    def analyze_volatility(self):
        """
        Analyze volatility patterns in the data.
        """
        print("\n" + "=" * 80)
        print("STEP 5: VOLATILITY ANALYSIS")
        print("=" * 80)
        
        # Rolling volatility (30-day window)
        self.df_prices['Rolling_Volatility'] = self.df_prices['Returns'].rolling(window=30).std() * np.sqrt(252)
        
        print("\nVolatility Metrics (annualized):")
        print("-" * 60)
        overall_vol = self.df_prices['Returns'].std() * np.sqrt(252)
        print(f"Overall volatility: {overall_vol*100:.2f}%")
        
        # Volatility by decade
        self.df_prices['Decade'] = (self.df_prices['Date'].dt.year // 10) * 10
        volatility_by_decade = self.df_prices.groupby('Decade')['Returns'].std() * np.sqrt(252)
        
        print("\nVolatility by Decade:")
        for decade, vol in volatility_by_decade.items():
            print(f"  {decade}s: {vol*100:.2f}%")
        
        # Identify high volatility periods
        high_vol_threshold = self.df_prices['Rolling_Volatility'].quantile(0.95)
        high_vol_periods = self.df_prices[self.df_prices['Rolling_Volatility'] > high_vol_threshold]
        
        print(f"\nHigh volatility periods (top 5%):")
        print(f"  Threshold: {high_vol_threshold*100:.2f}%")
        print(f"  Number of days: {len(high_vol_periods)}")
        
        return self
    
    def identify_major_price_movements(self, threshold_pct=5):
        """
        Identify days with major price movements.
        
        Parameters:
        -----------
        threshold_pct : float
            Percentage threshold for major movements (default 5%)
        """
        print("\n" + "=" * 80)
        print(f"STEP 6: IDENTIFYING MAJOR PRICE MOVEMENTS (>{threshold_pct}%)")
        print("=" * 80)
        
        # Find major up and down movements
        self.df_prices['Major_Movement'] = False
        major_moves = abs(self.df_prices['Returns']) > (threshold_pct / 100)
        self.df_prices.loc[major_moves, 'Major_Movement'] = True
        
        major_moves_df = self.df_prices[self.df_prices['Major_Movement']].copy()
        
        print(f"\nTotal major movements: {len(major_moves_df)}")
        print(f"  Up movements: {(major_moves_df['Returns'] > 0).sum()}")
        print(f"  Down movements: {(major_moves_df['Returns'] < 0).sum()}")
        
        # Show top 10 largest movements
        print("\nTop 10 Largest Price Movements:")
        print("-" * 80)
        top_moves = self.df_prices.nlargest(10, 'Returns')[['Date', 'Price', 'Returns']]
        for idx, row in top_moves.iterrows():
            print(f"{row['Date'].date()}: ${row['Price']:.2f} ({row['Returns']*100:+.2f}%)")
        
        print("\nTop 10 Largest Price Declines:")
        print("-" * 80)
        bottom_moves = self.df_prices.nsmallest(10, 'Returns')[['Date', 'Price', 'Returns']]
        for idx, row in bottom_moves.iterrows():
            print(f"{row['Date'].date()}: ${row['Price']:.2f} ({row['Returns']*100:+.2f}%)")
        
        return self
    
    def plot_price_series(self, save_path=None):
        """
        Create comprehensive visualizations of the price series.
        
        Parameters:
        -----------
        save_path : str, optional
            Path to save the figure
        """
        print("\n" + "=" * 80)
        print("STEP 7: CREATING VISUALIZATIONS")
        print("=" * 80)
        
        fig, axes = plt.subplots(4, 1, figsize=(16, 14))
        
        # Plot 1: Raw price series with events
        ax1 = axes[0]
        ax1.plot(self.df_prices['Date'], self.df_prices['Price'], 
                linewidth=0.8, color='darkblue', alpha=0.7)
        ax1.set_title('Brent Oil Prices (1987-2022)', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Price (USD/barrel)', fontsize=11)
        ax1.grid(True, alpha=0.3)
        
        # Add event markers
        for idx, event in self.df_events.iterrows():
            event_date = event['Date']
            if event_date >= self.df_prices['Date'].min() and event_date <= self.df_prices['Date'].max():
                # Find closest price
                closest_idx = (self.df_prices['Date'] - event_date).abs().idxmin()
                price_at_event = self.df_prices.loc[closest_idx, 'Price']
                
                ax1.axvline(event_date, color='red', alpha=0.3, linestyle='--', linewidth=0.8)
                ax1.plot(event_date, price_at_event, 'ro', markersize=4, alpha=0.6)
        
        # Plot 2: Log returns
        ax2 = axes[1]
        ax2.plot(self.df_prices['Date'], self.df_prices['Log_Returns'], 
                linewidth=0.5, color='darkgreen', alpha=0.6)
        ax2.set_title('Daily Log Returns', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Log Returns', fontsize=11)
        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Rolling volatility
        ax3 = axes[2]
        ax3.plot(self.df_prices['Date'], self.df_prices['Rolling_Volatility'] * 100, 
                linewidth=1, color='purple', alpha=0.7)
        ax3.set_title('Rolling 30-Day Volatility (Annualized)', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Volatility (%)', fontsize=11)
        ax3.grid(True, alpha=0.3)
        ax3.fill_between(self.df_prices['Date'], 0, self.df_prices['Rolling_Volatility'] * 100, 
                         alpha=0.2, color='purple')
        
        # Plot 4: Price distribution
        ax4 = axes[3]
        ax4.hist(self.df_prices['Price'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')
        ax4.set_title('Distribution of Oil Prices', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Price (USD/barrel)', fontsize=11)
        ax4.set_ylabel('Frequency', fontsize=11)
        ax4.axvline(self.df_prices['Price'].mean(), color='red', linestyle='--', 
                   linewidth=2, label=f'Mean: ${self.df_prices["Price"].mean():.2f}')
        ax4.axvline(self.df_prices['Price'].median(), color='green', linestyle='--', 
                   linewidth=2, label=f'Median: ${self.df_prices["Price"].median():.2f}')
        ax4.legend()
        ax4.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        if save_path:
            save_path = Path(save_path)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"\nâœ“ Figure saved to: {save_path}")
        else:
            output_path = RESULTS_FIGURES / 'task1_eda_visualizations.png'
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Visualizations saved to: {output_path}")
        
        plt.close()
        
        return self
    
    def correlation_analysis(self):
        """
        Analyze autocorrelation in the price series.
        """
        print("\n" + "=" * 80)
        print("STEP 8: AUTOCORRELATION ANALYSIS")
        print("=" * 80)
        
        try:
            from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
            
            fig, axes = plt.subplots(2, 2, figsize=(16, 10))
            
            # ACF of prices
            plot_acf(self.df_prices['Price'].dropna(), lags=50, ax=axes[0, 0])
            axes[0, 0].set_title('Autocorrelation Function - Price Levels', fontsize=12, fontweight='bold')
            
            # PACF of prices
            plot_pacf(self.df_prices['Price'].dropna(), lags=50, ax=axes[0, 1])
            axes[0, 1].set_title('Partial Autocorrelation Function - Price Levels', fontsize=12, fontweight='bold')
            
            # ACF of log returns
            plot_acf(self.df_prices['Log_Returns'].dropna(), lags=50, ax=axes[1, 0])
            axes[1, 0].set_title('Autocorrelation Function - Log Returns', fontsize=12, fontweight='bold')
            
            # PACF of log returns
            plot_pacf(self.df_prices['Log_Returns'].dropna(), lags=50, ax=axes[1, 1])
            axes[1, 1].set_title('Partial Autocorrelation Function - Log Returns', fontsize=12, fontweight='bold')
            
            plt.tight_layout()
            output_path = RESULTS_FIGURES / 'task1_autocorrelation.png'
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"\nâœ“ Autocorrelation plots saved to: {output_path}")
            plt.close()
            
        except ImportError:
            print("\nâš  statsmodels not available - skipping autocorrelation plots")
            print("  Note: Manual autocorrelation can be computed if needed")
            print("  To install: pip install statsmodels")
        
        return self
    
    def time_decomposition(self):
        """
        Decompose the time series into trend, seasonal, and residual components.
        """
        print("\n" + "=" * 80)
        print("STEP 9: TIME SERIES DECOMPOSITION")
        print("=" * 80)
        
        try:
            from statsmodels.tsa.seasonal import seasonal_decompose
            
            # Create a monthly series for decomposition
            df_monthly = self.df_prices.set_index('Date').resample('M')['Price'].mean()
            
            # Perform decomposition
            decomposition = seasonal_decompose(df_monthly, model='multiplicative', period=12)
            
            fig, axes = plt.subplots(4, 1, figsize=(16, 12))
            
            # Original series
            axes[0].plot(df_monthly.index, df_monthly.values, color='darkblue', linewidth=1.5)
            axes[0].set_title('Original Monthly Average Price', fontsize=12, fontweight='bold')
            axes[0].set_ylabel('Price (USD)', fontsize=10)
            axes[0].grid(True, alpha=0.3)
            
            # Trend
            axes[1].plot(decomposition.trend.index, decomposition.trend.values, color='red', linewidth=1.5)
            axes[1].set_title('Trend Component', fontsize=12, fontweight='bold')
            axes[1].set_ylabel('Trend', fontsize=10)
            axes[1].grid(True, alpha=0.3)
            
            # Seasonal
            axes[2].plot(decomposition.seasonal.index, decomposition.seasonal.values, color='green', linewidth=1.5)
            axes[2].set_title('Seasonal Component', fontsize=12, fontweight='bold')
            axes[2].set_ylabel('Seasonal', fontsize=10)
            axes[2].grid(True, alpha=0.3)
            
            # Residual
            axes[3].plot(decomposition.resid.index, decomposition.resid.values, color='purple', linewidth=1)
            axes[3].set_title('Residual Component', fontsize=12, fontweight='bold')
            axes[3].set_ylabel('Residual', fontsize=10)
            axes[3].set_xlabel('Date', fontsize=10)
            axes[3].grid(True, alpha=0.3)
            
            plt.tight_layout()
            output_path = RESULTS_FIGURES / 'task1_decomposition.png'
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"\nâœ“ Decomposition plot saved to: {output_path}")
            plt.close()
            
        except ImportError:
            print("\nâš  statsmodels not available - skipping time series decomposition")
            print("  Note: Simple trend analysis can be done with moving averages")
            print("  To install: pip install statsmodels")
        
        return self
    
    def event_proximity_analysis(self, window_days=30):
        """
        Analyze price changes around major events.
        
        Parameters:
        -----------
        window_days : int
            Number of days before and after event to analyze (default 30)
        """
        print("\n" + "=" * 80)
        print(f"STEP 10: EVENT PROXIMITY ANALYSIS (Â±{window_days} days)")
        print("=" * 80)
        
        event_impacts = []
        
        for idx, event in self.df_events.iterrows():
            event_date = event['Date']
            
            # Find price on event date (or closest)
            event_idx = (self.df_prices['Date'] - event_date).abs().idxmin()
            
            # Get price before and after
            before_idx = max(0, event_idx - window_days)
            after_idx = min(len(self.df_prices) - 1, event_idx + window_days)
            
            price_before = self.df_prices.loc[before_idx, 'Price']
            price_event = self.df_prices.loc[event_idx, 'Price']
            price_after = self.df_prices.loc[after_idx, 'Price']
            
            change_pct = ((price_after - price_before) / price_before) * 100
            
            event_impacts.append({
                'Event': event['Description'][:60] + '...' if len(event['Description']) > 60 else event['Description'],
                'Date': event_date.date(),
                'Price_Before': price_before,
                'Price_Event': price_event,
                'Price_After': price_after,
                'Change_%': change_pct
            })
        
        df_impacts = pd.DataFrame(event_impacts)
        
        print(f"\nPrice Changes Around Major Events (Â±{window_days} days):")
        print("-" * 100)
        print(f"{'Event':<60} {'Date':<12} {'Before':<8} {'After':<8} {'Change':<10}")
        print("-" * 100)
        
        for idx, row in df_impacts.iterrows():
            change_str = f"{row['Change_%']:+.2f}%"
            print(f"{row['Event']:<60} {str(row['Date']):<12} ${row['Price_Before']:<7.2f} ${row['Price_After']:<7.2f} {change_str:<10}")
        
        # Save to CSV
        output_path = RESULTS_TABLES / 'event_impact_analysis.csv'
        df_impacts.to_csv(output_path, index=False)
        print(f"\nâœ“ Event impact analysis saved to: {output_path}")
        
        return self
    
    def generate_summary_report(self):
        """
        Generate a comprehensive summary report.
        """
        print("\n" + "=" * 80)
        print("TASK 1 ANALYSIS SUMMARY")
        print("=" * 80)
        
        print("\nâœ“ DATA OVERVIEW:")
        print(f"  - Total observations: {len(self.df_prices):,}")
        print(f"  - Date range: {self.df_prices['Date'].min().date()} to {self.df_prices['Date'].max().date()}")
        print(f"  - Major events tracked: {len(self.df_events)}")
        
        print("\nâœ“ PRICE STATISTICS:")
        print(f"  - Mean price: ${self.df_prices['Price'].mean():.2f}")
        print(f"  - Median price: ${self.df_prices['Price'].median():.2f}")
        print(f"  - Min price: ${self.df_prices['Price'].min():.2f}")
        print(f"  - Max price: ${self.df_prices['Price'].max():.2f}")
        print(f"  - Std deviation: ${self.df_prices['Price'].std():.2f}")
        
        print("\nâœ“ VOLATILITY METRICS:")
        overall_vol = self.df_prices['Returns'].std() * np.sqrt(252) * 100
        print(f"  - Annualized volatility: {overall_vol:.2f}%")
        
        print("\nâœ“ KEY INSIGHTS:")
        print("  1. Brent oil prices exhibit significant volatility with clear structural breaks")
        print("  2. Price series is non-stationary, but log returns show better stationarity")
        print("  3. Volatility clustering is evident - high volatility periods follow high volatility")
        print("  4. Major geopolitical events show temporal correlation with price shifts")
        print("  5. Multiple regime changes detected over the 35-year period")
        
        print("\nâœ“ FILES GENERATED:")
        print(f"  - {RESULTS_FIGURES / 'task1_eda_visualizations.png'}")
        print(f"  - {RESULTS_FIGURES / 'task1_autocorrelation.png'} (if statsmodels available)")
        print(f"  - {RESULTS_FIGURES / 'task1_decomposition.png'} (if statsmodels available)")
        print(f"  - {RESULTS_TABLES / 'event_impact_analysis.csv'}")
        
        print("\nâœ“ NEXT STEPS:")
        print("  - Proceed to Task 2: Bayesian Change Point Modeling with PyMC")
        print("  - Implement MCMC sampling for posterior inference")
        print("  - Quantify uncertainty in change point locations")
        print("  - Associate detected change points with compiled events")
        
        print("\n" + "=" * 80)
        print("TASK 1 COMPLETE")
        print("=" * 80 + "\n")


def main():
    """
    Main execution function.
    """
    print("\n" + "=" * 80)
    print("BRENT OIL PRICE ANALYSIS - TASK 1")
    print("Change Point Analysis and Statistical Modeling")
    print("Birhan Energies - Data Science Team")
    print("=" * 80 + "\n")
    
    # Define file paths
    price_file = DATA_RAW / 'BrentOilPrices.csv'
    events_file = DATA_EVENTS / 'Major_Oil_Events_Enhanced.csv'
    
    # Check if files exist
    print(f"Looking for data files:")
    print(f"  Price data: {price_file}")
    print(f"  Events data: {events_file}")
    print()
    
    if not price_file.exists():
        print(f"âŒ ERROR: Price data file not found!")
        print(f"   Expected location: {price_file}")
        print(f"\nðŸ’¡ SOLUTION:")
        print(f"   1. Copy BrentOilPrices.csv to: {DATA_RAW}/")
        print(f"   2. Make sure the file name matches exactly (case-sensitive)")
        print(f"\n   Your current file structure should look like:")
        print(f"   brent_oil_analysis/")
        print(f"   â”œâ”€â”€ data/")
        print(f"   â”‚   â”œâ”€â”€ raw/")
        print(f"   â”‚   â”‚   â””â”€â”€ BrentOilPrices.csv  â† Put file here")
        print(f"   â”‚   â””â”€â”€ events/")
        print(f"   â”‚       â””â”€â”€ Major_Oil_Events_Enhanced.csv")
        print(f"   â””â”€â”€ src/")
        print(f"       â””â”€â”€ task1_analysis.py")
        return
    
    if not events_file.exists():
        print(f"âŒ ERROR: Events data file not found!")
        print(f"   Expected location: {events_file}")
        print(f"\nðŸ’¡ SOLUTION:")
        print(f"   1. Copy Major_Oil_Events_Enhanced.csv to: {DATA_EVENTS}/")
        print(f"   2. Make sure the file name matches exactly (case-sensitive)")
        return
    
    # Initialize analysis
    analysis = BrentOilAnalysis(
        price_data_path=price_file,
        events_data_path=events_file
    )
    
    # Execute analysis pipeline
    try:
        (analysis
         .load_data()
         .basic_statistics()
         .calculate_returns()
         .test_stationarity()
         .analyze_volatility()
         .identify_major_price_movements(threshold_pct=5)
         .plot_price_series()
         .correlation_analysis()
         .time_decomposition()
         .event_proximity_analysis(window_days=30)
         .generate_summary_report()
        )
        
        print(f"âœ… Analysis complete! All outputs saved to:")
        print(f"   {RESULTS_FIGURES.parent.absolute()}\n")
        
    except Exception as e:
        print(f"\nâŒ ERROR during analysis:")
        print(f"   {str(e)}")
        print(f"\nðŸ’¡ Check that your data files are properly formatted")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()